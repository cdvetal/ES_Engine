# ES Engine

This tool allows the use of a CMA-ES (Covariance Matrix Adaptation Evolution Strategy) algorithm to evolve renderings to approximate a certain class of the imagenet dataset. It relies on the use of pre-trained classifiers to guide the evolutionary process by calculating the reward value based on the certainty of these classifiers on the desired class. 
It is also possible to use the CLIP to guide the evolutionary process by calculating the cosine similarity between the encodings of the desired prompts and the encodings of the generated renderings. It can be use a combination of both methods by controlling the influence of the CLIP. 


## Rendering system
Each image is generated by a "renderer". Each "renderer" is located in the render subdirectory and can be visualized using the test_renderer.py script which output a sample image using each of the available "renderers". Each renderer is just a module that contains a render function:

```
# ind: array of real vectors, img_size: size of the desired output image
def render(ind, img_size):
    ...
    return img
```

The numpy array ind is a variable list of vectors and img_size is the dimensions of the output image in pixels. The output of the renderer should be PIL Image so it can be easily manipulated and used by the scoring system. Depending of the renderer, the size of the input vector may change. For example, in the pylinhas, each vector is a list of length 8 which is used to create a line. If the increase the number of vector we increase the number of lines. However, other renderers may require a larger vector for each line. However, this information is stored in the genotype_size variable which defines the size of each vector.
The VQGAN renderer is a special case of renderer. It is used to generate images using the VQ-VAE (Vector Quantized Variational Autoencoder) model, so its genotype_size is calculated based on the multiplication of the shape of the VQ-GAN input size, which is then reshaped and used as input to the model.

### Current renderers
- **pylinhas** - Draws using a set of colored lines
- **chars** - Draws using characters
- **organic** - Draws using organic shapes
- **thinorg** - Similar to organic but with thinner lines
- **vqgan** - Using the VQ-VAE model to generate images


## Scoring system
The scoring system is based on the use of pre-trained classifiers. Each classifier is located in the classifiers subdirectory and can be visualized using the test_classifier.py script. Each classifier contains three important features, a preprocessing function, a scoring function and a target size. The first one is a function that applies the required preprocess to the input image. The second one is a functions that receives the preprocessed image and returns a score. Lastly, the target_size is a set of two values that specify the size of their input. The score value they return is a real number between 0 and 1 and it's used to calculate the reward value of the individual. Multiple classifiers can be used where the reward value is calculated based on the sum of the scores for each classifier.


## Usage
This tool is able to work without any arguments as they have a pre-defined configuration file where all the required information is stored. However, using the command line arguments it is possible to change these values.

Examples:

```python es_engine.py```

```python es_engine.py --img-size 1024 --target-class goldfish```

```python es_engine.py --random-seed 1 --n-gens 200 --renderer organic --img-size 112 --networks inceptionv3,vgg16,xception,mobilenet,efficientnetb4,efficientnetb0 --target-class hummingbird```

- [Command Line Arguments](#command-line-arguments)
  - [--random-seed](#--random-seed)
  - [--save-folder](#--save-folder)
  - [--n-gens](#--n-gens)
  - [--pop-size](#--pop-size)
  - [--save-all](#--save-all)
  - [--verbose](#--verbose)
  - [--num-lines](#--num-lines)
  - [--num-cols](#--num-cols)
  - [--renderer](#--renderer)
  - [--img-size](#--img-size)
  - [--target-class](#--target-class)
  - [--networks](#--networks)
  - [--target-fit](#--target-fit)
  - [--from-checkpoint](#--from-checkpoint)
  - [--init-mu](#--init-mu)
  - [--init-sigma](#--init-sigma)
  - [--sigma](#--sigma)
  - [--clip-influence](#--clip-influence)
  - [--clip-model](#--clip-model)
  

### --random-seed
Choose the random seed. Default is None. Example:

```python es_engine --random-seed 1909```

### --save-folder
Directory to experiment outputs. Default is 'experiments'. Example:

```python es_engine --save-folder saves```

### --n-gens
Maximum generations. Default is 100. Example:

```python es_engine --n-gens 150```

### --pop-size
Population size. Default is 40. Example:

```python es_engine --pop-size 50```

### --save-all
Save all Individual images. Default is False. Example:

```python es_engine --save-all```

### --verbose
Verbose. Default is False. Example:

```python es_engine --verbose```

### --num-lines
Number of lines. Default is 17. Example:

```python es_engine --num-lines 5```

### --num-cols
Number of columns. Default is 8. Example:

```python es_engine --num-cols 5```

### --renderer
Choose the renderer. Default is 'pylinhas'. Example:

```python es_engine --renderer chars```

### --img-size
Image dimensions during testing. Default is 256. Example:

```python es_engine --img-size 512```

### --target-class
Which target classes to optimize. Default is 'birdhouse'. Example:

```python es_engine --target-class goldfish```

### --networks
Comma separated list of networks. Default is 'mobilenet,vgg16'. Example:

```python es_engine --networks vgg16,vgg19,mobilenet```

### --target-fit
Target fitness stopping criteria. Default is 0.999. Example:

```python es_engine --target-fit 0.9```

### --from-checkpoint
Checkpoint file from which you want to continue evolving. Default is None. Example:

```python es_engine --from-checkpoint Experiment_name.pkl```

### --init-mu
Mean value for the initialization of the population. Default is 0.5. Example:

```python es_engine --init-mu 1909```

### --init-sigma
Standard deviation value for the initialization of the population. Default is 0.25. Example:

```python es_engine --init-sigma 0.2```

### --sigma
The initial standard deviation of the distribution. Default is 0.2. Example:

```python es_engine --sigma 0.1```

### --clip-influence
The influence CLIP has in the generation (0.0 - 1.0). Default is 0.0. Example:

```python es_engine --clip-influence 0.5```

### --clip-model
Name of the CLIP model to use. Default is 'ViT-B/32'. Example:

```python es_engine --clip-model RN50x16```

### --clip-prompts
CLIP prompts to use for the generation. Default is the target class. Example:

```python es_engine --clip-prompts "a dog eating cereal"```


## TODO

- [x] Pylinhas, chars, organic and thinorg renderers working
- [x] Add support for multiple classifiers
- [x] Add support for VQGAN
- [ ] Test VQGAN
- [ ] Add support to GPU and CPU
- [ ] Use image as input
- [ ] Add more renderers

